title:: Transformer架构的核心公式其实类似于数学期望，理解起来也不复杂，但为什么这个模型这么强呢？ - 知乎 (highlights)
author:: [[首页]]
full-title:: "Transformer架构的核心公式其实类似于数学期望，理解起来也不复杂，但为什么这个模型这么强呢？ - 知乎"
category:: #articles
url:: https://www.zhihu.com/question/580810624/answer/2979260071

- Highlights first synced by [[Readwise]] [[Sep 10th, 2023]]
	- 这就是深度神经网络中深度的含义。在这个例子中，我们通过更多的卷积操作，把卷积网络堆叠的更深，以此来让它有机会捕捉“长距离依赖”。
	  
	  换言之，卷积网络主要依靠深度来捕捉长距离依赖。 ([View Highlight](https://instapaper.com/read/1632178286/23053755))
	- 但这个过程太间接了，因为信息在网络中实际传播了太多层。究竟哪些信息被保留，哪些被丢弃了，弄不清楚。
	  
	  从实践经验来看，卷积网络捕捉长依赖的能力非常弱。这也是为什么在大多数需要长依赖关系建模的场景中，CNN用的并不多的原因。 ([View Highlight](https://instapaper.com/read/1632178286/23053758))
	- 容易看出，在这个过程中，因为后一个词的计算需要用到前一个词的输出结果，所以理论上任何两个词的依赖RNN都能捕捉到。
	  
	  以信息流的方法来看，图中绿色箭头表明信息的流动方向。容易看到，不论是相隔多远的词，它们的信息一定会相聚在某一步计算中。
	  
	  上述是理论上的情况。实际中，因为RNN训练过程中容易出现梯度消失或梯度爆炸，所以它实际上很难把长依赖捕捉的比较好。有一些研究表明，不论是何种改进的RNN（如LSTM、GRU），它们一般捕捉长依赖的极限也就20个词的左右能力。 ([View Highlight](https://instapaper.com/read/1632178286/23053765))
	- 不用看Transformer的公式，单看它的计算逻辑就能发现，它在计算任意一个词的新表征（特征）时，同时用到了其它所有词的信息。 ([View Highlight](https://instapaper.com/read/1632178286/23053772))
	- 题主提到的数学期望其实是一个“加权和”。它是在计算得到所有attention score后，以加权和的形式来计算某一个词的新表征。
	  
	  例如，对于单词I，它的新表征L(I)就是其它所有单词Value的加权和。所以仅仅只看这一点，I的新表征中就能用到其它所有单词的信息。因此，没有距离的概念，也就没有长依赖的问题。 ([View Highlight](https://instapaper.com/read/1632178286/23053775))