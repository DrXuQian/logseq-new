- https://onnxruntime.ai/docs/performance/olive.html
- ## Overview
- [Olive](https://github.com/microsoft/Olive)Â is an easy-to-use hardware-aware model optimization tool that composes industry-leading techniques across model compression, optimization, and compilation. It works with ONNX Runtime as an E2E inference optimization solution.
- Given a model and targeted hardware, Olive composes the best suitable optimization techniques to output the most efficient model(s) and runtime configurations for inferencing with ONNX Runtime, while taking a set of constraints such as accuracy and latency into consideration. Techniques Olive has integrated include ONNX Runtime Transformer optimizations, ONNX Runtime performance tuning, HW-dependent tunable post training quantization, quantize aware training, and more. Olive is the recommended tool for model optimization for ONNX Runtime.
- ## Transformer optimizer
- This is from onnxruntime:
	- https://onnxruntime.ai/docs/performance/transformers-optimization.html
	- https://github.com/microsoft/onnxruntime/blob/rel-1.9.0/onnxruntime/python/tools/transformers/fusion_attention.py#L281-L441
- Help with:
	- - ONNX Runtime does not yet have transformer-specific graph optimization enabled
	- - The model has inputs with dynamic axis, which blocks some optimizations from being applied by ONNX Runtime due to shape inference.
	- - Experimenting with disabling or enabling some fusions to evaluate impact on performance or accuracy.
		- Fusion transformer MHA, SKIPLAYERNORM
		- ![image.png](../assets/image_1694045373302_0.png)