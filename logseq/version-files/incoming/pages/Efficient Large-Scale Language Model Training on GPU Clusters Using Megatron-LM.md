### Source
	- https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/
- Overview
	- ![image.png](../assets/image_1704549390993_0.png)
- ### pipeline parallelism compared with Gpipe
	- #Gpipe
		- ![image.png](../assets/image_1704549907454_0.png)
		- æ³¨æ„ï¼Œæ¯æ¬¡ä¸€ä¸ªbatchçš„å‰å‘å’Œåå‘ç»“æŸä¹‹åï¼Œéœ€è¦åŒæ­¥ä¸€ä¸‹optimizerçš„ä¿¡æ¯ï¼Œè¿™æ ·ä¿è¯äº†optimizerçš„æ­£ç¡®æ€§ã€‚
		- The bubble is minimized when number of microbatchs >> number of GPUs for pipeline parallelism
		- **æ—¶é—´åˆ†æ**
			- è¿™ç§æ–¹æ³•çš„bubbleçš„æ—¶é—´å·®ä¸å¤šéœ€è¦$$t_{pb} = (p-1)*(t_f + t_b)$$ï¼Œè¿™ä¸ªç®€å•æ¥çœ‹å°±æ˜¯åœ¨å…¨éƒ¨çš„forwardä¹‹å‰æœ‰$$p-1$$ä¸ªç©ºæ ¼ã€‚åœ¨æœ€åçš„backwardä¹‹åè¿˜æœ‰$$p-1$$ä¸ªç©ºæ ¼ã€‚å…¶ä¸­$$t_f, t_b$$æ˜¯ä¸€ä¸ªmicrobatchçš„å‰å‘å’Œåå‘çš„æ—¶é—´å¼€é”€ã€‚
			- ç†æƒ³çš„æ—¶é—´å¼€é”€æ˜¯$$t_{id} = m * (t_f + t_b)$$ï¼Œ è¿™é‡Œçš„$$m$$æ˜¯micro batchçš„æ•°é‡ã€‚
			- æ‰€ä»¥å¯ä»¥å¾—å‡ºï¼š$$bubble\ time\ fraction = t_{pb}/t_{id} = (p - 1)/m$$
		-
	- ![image.png](../assets/image_1704549959705_0.png)
	- Pipeline Dream flush:
		- Summary:
			- One such way is 1F1B scheduling proposed by PipeDream, wherein during the forward pass, initially, the micro-batches are allowed to flow forward until the last group receives the first micro-batch. But then the backward propagation of the first batch starts, and from then, a forward pass is always accompanied by a backward pass, hence the name 1F1B.
			- ç¬¬ä¸€ä¸ªmicrobatchå‰å‘ç»“æŸä¹‹åç«‹é©¬å¼€å§‹åå‘ï¼Œç„¶åå°±æ˜¯interleaveçš„
		- upper half of figure4, é™åˆ¶äº†åŒæ—¶å‘ç”Ÿçš„microbatchçš„æ•°é‡ï¼Œå¦‚å›¾æ‰€ç¤ºï¼ŒåŸæ¥æ‰€æœ‰çš„microbatchéƒ½ä¼šåšå®Œinferenceä¹‹åå¼€å§‹backwardã€‚
		- åœ¨å›¾4ä¸­ï¼Œdevice4 åœ¨åšå®Œç¬¬ä¸€ä¸ªmicrobatchçš„forwardä¹‹åï¼Œå¹¶æ²¡æœ‰å¼€å§‹åä¸€ä¸ªmicrobatchçš„forwardï¼Œè€Œæ˜¯å»åšäº†ç¬¬ä¸€ä¸ªmicrobatchçš„backwardã€‚
		- æ—¶é—´ä¸Šè·ŸGpipeæ˜¯ä¸€è‡´çš„ï¼Œä½†æ˜¯ï¼Œå› ä¸ºåŒæ—¶æœ€å¤šè®¡ç®—4ä¸ªmicrobatchçš„forwardå’Œbackwardï¼Œæ‰€ä»¥åªéœ€è¦ä¿ç•™4ä¸ªmicrobatchçš„intermediate activationsã€‚ä½†æ˜¯Gpipeæ–¹æ¡ˆå› ä¸ºæ¯æ¬¡è¦å…ˆç®—å®Œæ‰€æœ‰çš„microbatchçš„forwardï¼Œç„¶åè®¡ç®—backwardï¼Œæ‰€ä»¥éœ€è¦ä¿ç•™8ä¸ªã€‚å¦‚æœmicrobatchçš„æ•°é‡ç‰¹åˆ«å·¨å¤§ï¼Œ å†…å­˜çš„é—®é¢˜ä¼šæ›´æ˜æ˜¾ã€‚
	- Schedule with interleaved stages:
		- ä¸ºäº†å‡å°‘bubbleï¼Œæ¯ä¸ªdeviceå¯ä»¥åœ¨æ¯ä¸ªè‡ªå·±çš„localçš„modelä¸Šåˆ‡åˆ†å‡ºæ›´å°çš„layerï¼Œå«åšmodel rankã€‚åŸæ¥ä½ çš„pipeline dream flushæ˜¯(device 1 has layers 1-4, device 2 has layers 5-8, and so on).
		- èªæ˜çš„è§£å†³æ–¹æ¡ˆï¼ŒæŠŠ1ï¼Œ2ï¼Œ9ï¼Œ10 åˆ’åˆ†ç»™device1, device2æ˜¯3ï¼Œ4ï¼Œ11ï¼Œ12ã€‚device3æ˜¯5ï¼Œ6ï¼Œ13ï¼Œ14ã€‚device4æ˜¯7ï¼Œ8ï¼Œ15ï¼Œ16ã€‚
		- è¿™æ ·ç²’åº¦æ›´ç»†äº†ï¼Œè®©ä¸åŒçš„deviceåœ¨ä¸¤ä¸ªstageéƒ½èƒ½å‚ä¸è¿ç®—ã€‚èƒ½å¤Ÿå‡å°‘bubbleåˆ°åŸæ¥çš„1/vï¼Œå…¶ä¸­væ˜¯stageçš„æ•°é‡ã€‚ä½†æ˜¯æœ‰é¢å¤–çš„commnicationçš„å¼€é”€ã€‚
		  id:: 6599639b-fcaf-4793-9819-7d3bb4a544b4
		- è¿™ç§ç­–ç•¥çš„bubble time:
			- å› ä¸ºæ¯ä¸ªdeviceæœ‰$$v$$ä¸ªstageï¼Œä¹Ÿå°±æ˜¯$$v$$ä¸ªmodel chunksã€‚é‚£ä¹ˆå‰å‘å’Œåå‘çš„æ—¶é—´ï¼Œåˆè¿›ä¸€æ­¥çš„å‡å°‘åˆ°äº†$$t_f/v$$ä»¥åŠ$$t_b/v$$ï¼Œé‚£å‰é¢å’Œåé¢çš„bubbleæ—¶é—´åº”è¯¥æ˜¯ï¼š
			- $$bubble\ time = (p-1)*(t_f+t_b)/v$$
- ### TP+MP+DP
	- {{embed ((6599577e-6a83-47c6-b045-1531839ae9d0))}}
	- Communication cost modelling is very hard. This is the first work to analyze performance interactions of different parallel dimensions.
	- #### Tensor and Pipeline Model Parallelism
		- Communication cost for tensor parallelism is high when multiple shards are not within the same GPU clusters. Need inter-server communication which can be real slow.
		- > Takeaway 1: When considering different forms of model parallelism, tensor model parallelism should generally be used up to degree ğ‘” when using ğ‘”-GPU servers, and then pipeline model parallelism can be used to scale up to larger models across servers
	- #### Data and Model Parallelism
		- > Takeaway 2: When using data and model parallelism, a total model-parallel size of ğ‘€ = ğ‘¡ Â· ğ‘ should be used so that the model's parameters and intermediate metadata fit in GPU memory; data parallelism can be used to scale up training to more GPUs.
	- #### Microbatch Size
		- > Takeaway 3: The optimal microbatch size ğ‘ depends on the throughput and memory footprint characteristics of the model, as well as the pipeline depth ğ‘, data-parallel size ğ‘‘, and batch size B.
	- #### Activation Recomputation
		- [[gradient checkpointing]]
		- > For most cases, checkpointing every 1 or 2 transformer layers is optimal
- ### Implementation
	- Communication optimization:
		- ![image.png](../assets/image_1704605138742_0.png)
		- å¦‚å›¾æ‰€ç¤ºï¼Œ1å’Œ2æ˜¯tensor parallelismï¼Œ1/2 å’Œ 3/4æ˜¯pipeline parallelismã€‚æ‰€ä»¥1å’Œ2æ“ä½œä¸åŒçš„tensorï¼ˆå¯¹äºMHAæ˜¯ä¸åŒçš„headï¼Œå¯¹äºMLPæ˜¯ä¸åŒçš„channelçš„activationï¼‰ï¼Œ1/2å’Œ3/4æ˜¯æ¨¡å‹ä¸åŒçš„vertical stageã€‚
		- å› ä¸ºtensor parallelismè¦æ±‚è¾“å…¥è¾“å‡ºæ˜¯å®Œæ•´çš„tensorï¼Œæ‰€ä»¥å†è·¨pipelineä¼ è¾“æ•°æ®æ—¶ï¼Œå³ä½¿åˆ‡åˆ†äº†ï¼Œä¹Ÿä¼šåœ¨é€šè¿‡inifibandä¼ è¾“ä¹‹å‰é€šè¿‡nvlink all-gatheræ¥ç»„åˆæˆå®Œæ•´æ•°æ®ã€‚è®ºæ–‡æå‡ºçš„ä¼˜åŒ–å°±æ˜¯ï¼Œä¸ç›´æ¥ä¼ è¾“gatherä¹‹åçš„å…¨éƒ¨æ•°æ®ï¼Œè€Œæ˜¯åªä¼ è¾“ä¸€éƒ¨åˆ†ï¼Œå¦‚å›¾ä¸Šæ‰€ç¤ºï¼Œgpu1å’Œgpu2æ¯ä¸ªå„ä¼ è¾“1/2çš„æ•°æ®ã€‚
		- ç„¶åå†æ¥æ”¶ç«¯é€šè¿‡all gatheræ¥è¿›è¡Œç»„åˆï¼Œç”±äºæ¥æ”¶ç«¯æ˜¯ç”¨nvlinkè¿›è¡Œé€šä¿¡ï¼Œæ‰€ä»¥ä¼šå¿«å¾ˆå¤šã€‚
- ### Experiments
	- Tensor parallel size should equal to number of nodes in one GPU server (8 for DGX A100)
	- The more microbatch is, the less bubble there are when using pipeline parallelism.
	- 10% after the optimization of communication (gather/scatter)
	- 10% after optimization of interleaved pipeline parallelism (less bubble).
-