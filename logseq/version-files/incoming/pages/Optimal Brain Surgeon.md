- https://arxiv.org/pdf/2004.14340.pdf
- https://zhuanlan.zhihu.com/p/656316235
- 对于一个训练好得模型，误差$$E$$是权重$$w$$的参数，假设达到了局部最优，也就是一阶导为0，此时的$$w$$为$$w_0$$
- 那么，把$$E$$在$$w$$处展开，可以得出：
	- ![image.png](../assets/image_1705281897452_0.png)
- 因为前面已经假设达到了局部最优，所以可以忽略等式右边的第二项，同时我们假设高阶无穷小可以忽略。那么可以得到：
	- ![image.png](../assets/image_1705281990318_0.png)
- 如果要剪枝某一个权重，也就是剪枝之后让 ![image.png](../assets/image_1705282781852_0.png)，其中$$e_q$$是一个向量，其中第$$q$$个元素是1，其他位置为0。$$w_q$$就是第$$q$$个权重。
- 直接剪枝某一个权重，会导致精度受到影响，所以为了补偿精度损失，需要调整其他权重，让$$E$$的改变最小。这个过程可以等价为最小化下面的公式([[拉格朗日乘子法]])：
	- ![image.png](../assets/image_1705282972801_0.png)
- 对于$$\delta w$$以及$$\lambda$$分别求导：
	- ![image.png](../assets/image_1705284003899_0.png)
- 因为$$H$$和$$H^{-1}$$是等价的，所以上面的公式可以推到出 ![image.png](../assets/image_1705284154003_0.png) ， 带入下面的公式，得出：
	- ![image.png](../assets/image_1705284057569_0.png)
	- 这是因为$$e_q^T*H_{-1}*e_q=H^{-1}_{qq}$$
- 将(5)带入 ![image.png](../assets/image_1705284154003_0.png) ，可以推到出：
	- ![image.png](../assets/image_1705284102494_0.png)
- 将公式(5), (6)带入公式(2)，可以得到：
	- ![image.png](../assets/image_1705284535319_0.png)
- 也就是说，我们需要prune的weight应该要满足让(7)最小。然后再prune之后，我们需要按照公式(6)调整其他权重。
- 上面的都是prune的场景，对于量化，我们可以通过对于(6)和(7)的简单修改得出：
	- ![image.png](../assets/image_1705284618248_0.png)
	- ![image.png](../assets/image_1705284626906_0.png)