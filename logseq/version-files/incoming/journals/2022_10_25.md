- DONE optimizing model size - Victor
  :LOGBOOK:
  CLOCK: [2022-10-25 Tue 09:57:23]--[2022-10-25 Tue 14:54:38] =>  04:57:15
  :END:
	- using int8
	- weight sparsity
		- compression ratio
- optimizing performance
	- relu type activation functions. ReLU is best due to activation sparsity, ReLUx/PReLU with per tensor also is next best without activation sparsity acceleration
	- DONE tanh, sigmoid and next best as there are special instructions for this in the DSP - Victor
	  :LOGBOOK:
	  CLOCK: [2022-10-25 Tue 09:56:14]--[2022-10-25 Tue 14:54:41] =>  04:58:27
	  :END:
	- channel dimension multiple of 16
	- HW dimension multiple of 4x4
	- DONE DW conv vs 2D conv - qianxu
	  :LOGBOOK:
	  CLOCK: [2022-10-25 Tue 09:56:22]--[2022-10-25 Tue 14:54:42] =>  04:58:20
	  :END:
	- nearest neighbor is better than bilinear interpolation
	- DONE depth to space shape limitations (can't remember what these are)
	  :LOGBOOK:
	  CLOCK: [2022-10-25 Tue 09:57:45]--[2022-10-25 Tue 14:54:42] =>  04:56:57
	  :END:
		- Sync with Murali
		- SE-pointer constraint:
			- output channel need to be multiplier of 16
	- DONE increase computational complexity to avoid memory BW bound layers (detail out calculation here) - Qianxu
	  :LOGBOOK:
	  CLOCK: [2022-10-25 Tue 09:58:15]--[2022-10-25 Tue 14:54:43] =>  04:56:28
	  :END:
	- DONE replace layernorm with "NoNorm" as is done in mobilebert - Victor
	  :LOGBOOK:
	  CLOCK: [2022-10-25 Tue 09:58:23]--[2022-10-25 Tue 14:54:43] =>  04:56:20
	  :END:
- [[PWL using trainable method]]
-