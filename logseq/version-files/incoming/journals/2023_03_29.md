- 13:49
	- L2 passes
		- L2 ops
			- with engine information
			- tensor with location information
		- - [ ] insert copy for network input and output (weight)
		  :LOGBOOK:
		  CLOCK: [2023-03-29 Wed 14:33:48]
		  :END:
		- determine multi-cluster strategy
			- special pass for implicit op or copy op
		- - [ ] add control edge to ensure the order is DFS order
		- temporal tiling
			- vertical fusion tiling
				- copy + any kind of op
				- any kind of op + copy
				- patterns specified by user
					- dot prod attn
			- isolated tiling
				- insert copy/slice/concat/tiled ops to graph
				- need to make sure the order is correct within the temporal tiled subgraph
					- ```
					          tiling of op:
					              Copy To DDR --> slice --> Copy To CMX --> t_op0 --> Copy To DDR --> Concat --> Copy To CMX
					                          --> slice --> Copy To CMX --> t_op1 --> Copy To DDR --/
					  ```
			- duplicate copy optimization
		- mark op on DDR
			- - [ ] need to insert copy and ensure the execution order is correct
			- duplicate copy optimization
		- stride copy optimization
			- keep control edge
		- memory allocation
	- 任务分解：
		- step1:
			- copy op to tasklst, no copy to DMATask conversion in instruction level
			- refactor of temporal tiling adapter, apply temporal tiling
				- operator, node cfg(temporal tiling strat) --> apply to graph
			- a adapter for L2 that ensure the order is reserved after fusion of operator and erase of operators
			- erase skip op after memory allocation
		- step 2:
			- insert copy in temporal tiling/vertical fusion
			- insert copy in mark op on DDR pass
			- DFS order ignore vertical fusion/temporal tiled ops
		- implicit operator optimization
			- fuse that into Conv DPU as IDU/ODU
				- reshape/odu transpose/permutecast
- 20:08
  collapsed:: true
	- script for today's meeting
	  id:: 64242a46-1bbf-4660-93fe-e7c2b6bb1a83
	  collapsed:: true
		- Basically, there are three optimization opportunities we see. First is vertical fusion, then implicit reshape. and finally DPU/Shave pipeline.
		- We profiled a single encoder block from Unet in stable diffusion and try turn on and off these optimization opportunities. And the profiling results using VPUEM are as shown in this chart.
		- As you can see in this first column, without optimization, the latency for a single block is 55ms, after vertical fusion, the latency dropped to 31ms, after adding implicit reshape, the latency further dropped to 26ms. After adding DPU/Shave pipelining, the latency further dropped to 21 ms.
		- Currently the profiling results still have some issue with profiling the softmax. We will re-run the whole network's profiling after this issue is fixed.
		- And for these three optimization opportunities, implicit reshape is most straight forward and I think is the easiest for VPUX to implement. And the vertical fusion is harder and pipelining of DPU/Shave operators depends on vertical fusion.
		- We did some analysis on the Blob compiled by Gaoxiong. And we find some optimization opportunities using implicit reshape.
		- Basically, when VPUX unroll the Matmul and convert the Matmul to Convolution. The input width of the convolution would always be 1. For example, a matmul with input [2048, 48] x [48, 256] would translate to convolution with [1,2048, 1, 48] NHWC x [256, 1, 1, 48]. A convolution with w==1 would have low compute efficiency since the DPU consumes 4x4 in HW dimension at once.
		- These operators are taken from the blob of VPUX. And we tried implicit reshape on these operators and profiled using VPUEM.
		- As you can see, for the first column, we have a convolution with 2048 x 1 as H and W. We can implicit reshape that to 128 x 16. And the task cycles decrease ~7000 and task eff increased 16%.
		-
	-