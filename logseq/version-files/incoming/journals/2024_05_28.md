- [[GPU]][[CUDA]]
	- questions:
		- 为什么GPU中需要这么多的[[warp]] scheduler？
		  collapsed:: true
			- 2个warp scheduler对应很个warp，当schedule其中一个warp的时候，可能会出现等待数据搬运到shared local memory的情况，这个时候可以通过大量的寄存器资源保存这个context。然后切换到别的warp执行。
			- {{embed ((65b1b063-97ef-4c3b-a1e2-82c9b5a8f502))}}
			- {{embed ((65b1b063-f9c2-432b-9de9-7dc69e9c08b5))}}
			- 类似的，在npu中，如果我们在inference的时候有并发的场景，可以切换到别的context，有点像是preemption，但是需要是无损的。
		- GPU内部每个warp执行的时候是独占资源吗？
		  collapsed:: true
			- 每个warp执行的时候是对应了一个或者多个[[SM]]。每个warp对应了32个thread，然后内部以SIMD为粒度执行。
				- 但是不同的blockidx的warp可以执行完全不同的thread，例子：
					- ```cuda
					  __global__ void mixedTasks(float* img, float* vecA, float* vecB, float* result, int imgSize, int vecSize) {
					      int idx = blockIdx.x * blockDim.x + threadIdx.x;
					  
					      // Task 1: 图像处理
					      if (blockIdx.x < 10) {  // 假设前10个block用于图像处理
					          if (idx < imgSize) {
					              float pixel = img[idx];
					              float filteredPixel = (pixel / 2) + 100;  // 简单的滤镜效果
					              img[idx] = filteredRoleftPx;
					          }
					      }
					      // Task 2: 向量加法
					      else {  // 假设后续的block用于向量加法
					          if (idx < vecSize) {
					              result[idx] = vecA[idx] + vecB[idx];
					          }
					      }
					  }
					  /*
					  在这个例子中：
					  前10个block可能专门处理图像数据，每个block中的线程（组织成warp）会读取图像数据，应用滤镜，并写回结果。
					  后续的block处理向量加法，其中每个线程（同样组织成warp）读取两个向量的对应元素，计算它们的和，并存储到结果数组中。
					  
					  这里的关键点是，尽管每个warp执行的是相同kernel内的指令，但由于他们属于不同的block，
					  所以实际执行的分支和操作可以完全不同。这种设计允许CUDA程序灵活地处理多种不同的任务,
					  同时充分利用GPU的并行计算能力。
					  */
					  ```
		- GPU中的branching？
		  collapsed:: true
			- CUDA编程中，`branch instruction`通常指的是诸如`if-else`语句或循环中的条件判断，这种情况下不同的线程可能会根据自己的数据或`threadIdx`（线程索引）执行不同的代码路径。当这种情况在一个warp中发生时，即使原本设计是所有线程同时执行相同的指令，线程间的执行路径可能会发散，导致不同线程执行不同的指令。
			- 在一个warp中，如果线程因为条件判断而需要执行不同的分支，GPU将会处理所有可能的分支，但在任何给定时刻，整个warp中的所有线程仍然只能执行一条指令。这意味着GPU将会序列化不同分支的执行，首先执行一条分支路径上的所有线程，然后执行另一条路径。尽管物理上是同时开始执行，但由于分支指令的存在，不同的线程可能会被迫等待其他线程完成他们的分支指令。
			- 这种现象被称为“warp发散”（warp divergence），它会降低warp的执行效率，因为不是所有的线程都能在每个时钟周期内有效工作。当一部分线程在执行一个分支时，其他不在该分支的线程将会闲置等待。
			- 为了尽量减少warp发散的影响，CUDA程序员通常会尽力优化代码，以确保在同一个warp中的线程能够尽可能地执行相同的指令流，或者通过算法设计来平衡不同线程的工作量，从而提高程序的整体性能和效率。
		- Block和[[warp]]分别映射到硬件上的什么单元？
		  collapsed:: true
			- 在CUDA架构中，[[warp]]是执行的基本单位，而block是==调度的基本单位==。当一个block被映射到一个SM上时，这个block中的所有warp也随之在这个SM上执行。这里的映射和执行机制如下：
			- **Block的调度**：当一个线程块（block）被创建时，CUDA的调度器会将整个block分配给一个流多处理器（SM）。这个block不会跨多个SM执行，整个block的执行完全在一个SM内部完成。
			- **Warp的执行**：在block内部，线程被进一步组织为warp，每个warp包含32个线程。SM中有一个或多个warp scheduler，负责管理和调度这些warp的执行。当block被调度到SM之后，block内的所有warp都将由这个SM的warp scheduler动态地调度执行。
		- Shared memory和l1的trade off?
			- 在turning和volta架构中，l1和local memory是共享同一块硬件单元。
			- 可以在编译选项中配置分别的大小。
			- shared memory比l1快一点？
- [[groq]]