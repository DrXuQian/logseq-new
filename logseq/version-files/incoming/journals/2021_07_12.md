- Take a look at all the network from Murali about dynamic shape problem and dynamic operations
	 - Bert and transformer can take various length of input sequence

	 - Average pooling would allow input to be dynamic for classifications

	 - LSTM and RNN operations can take input sequence one by one and the length can be any length actually

	 - Super resolution layers can actually take any kind of input

	 - As long as there is no specific layer or operation that require the input to be fixed, like full-connected layer with input flattened, the network can accept dynamic shapes.

- Glance at MKLDNN and how the convolution is done in MKLDNN
	 - how the computation block is split, memory layout

	 - how to choose between winograd and gemm
