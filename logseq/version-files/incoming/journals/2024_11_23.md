- script for interviewing for 建雄学院
	- 首先自我介绍。我是钱煦，我的研究方向主要集中在人工智能和通信系统。尤其是人工智能在移动端的应用。这次的课程我也将围绕这个话题展开。
	- 现在最热门的人工智能的话题可能就是大语言模型了，像是chatgpt啊，或者国内的文心一言，盘古，豆包等等。这些模型功能都很强大啊，像是我现在基本上工作中已经离不开了。大语言模型最大的特点是什么呢？就是大。可能是几百亿的参数量，光是模型的大小，就有几个G，甚至几十个G，几百个G。这就带来了一个问题，我们手机上或者我们笔记本上怎么运行这些模型呢？根本没办法对吧，因为手机上运行内存可能就是8个G，或者多一点的16G，32G。而且还要分给其他app，还有操作系统，对吧。你总不能用大语言模型的时候收不了微信了或者接不了电话了，对吧。
	- 这个时候，就引入了我今天讨论的话题，模型压缩。通过模型压缩，我们原来的几十个G的模型可能能缩小十几倍甚至更多，当然肯定不是无损的，压缩之后几十个M的模型可以运行了，但是效果会差一点，但是也够用了。举个可能不是很恰当的例子，就像是你手机拍摄的图片，原始的raw的文件，有十几M，那有的时候手机拍不了几张就满了。这个时间可以对图片做一些压缩，可能会有一些模糊，但是正常看，看不出来差别。
	- 模型压缩呢，就是类似的，只是我们是对神经网络进行压缩。通常我们用的方法就是下面这几种，量化，剪枝，还有蒸馏，低yi分解。。。今天主要就是讲量化和剪枝。
	- 首先可能介绍一下神经网络是什么，最简单的场景就是，我们认为神经网络就是一个函数，他拿到了我们提供给他的输入，比如说问他你是谁？然后他把这个输入转成一个数组，然后经过一系列的处理，输出另外一个数组输出出来，然后我们再把输出文本化。比如他回答，我是文心一言。这个中间的部分，就是神经网络。这个中间可能有非常多的参数，然后这些参数呢，就是构成了一个模型。因为有几百亿个这样的参数，所以最后模型才那么大。
	- 这些参数呢，在电脑存储的时候，都是用fp32格式存储的。这里的另外一个知识点就是数据格式，再神经网络领域，常用的数据格式有这些：
		- fp32/fp16/fp8/fp4
			- ![image.png](../assets/image_1732363002136_0.png)
		- int8/int4/int2/...
			- 线性，均匀
	- 我们再举一个例子，比如说最常见的无理数，pi，我们把他写下来就是3.14159265358979323846... 电脑上存储的我们肯定不能无限存储所有的pi，那我们硬盘再大也存储不下。一般我们用fp32 来表示，精度就已经足够了，这个时候fp32表示就是3.141592653589793,如果你去硬盘上去看的化，能看到32个bit分别是：
		- **符号位（Sign）**：`0` （正数）
		- **指数位（Exponent）**：`10000000`
		- **尾数（Fraction）**：`10010010000111111011011`
	- 量化是什么意思呢，就是我们也不需要这么截断了，就直接表示成一个整数，那就是3。这个就叫做量化，那这个跟神经网络压缩有什么关系呢？很简单，原始fp32我们需要多个bit表示，需要32个bit。然后量化之后呢，如果我们用int8表示，那我们的大小就变成了原来的1/4。当然，在这个场景下面，我们甚至只需要2-bit，这个时候，就压缩到了原来的1/16。然后我们也引入一个新的概念，叫压缩率。。。
	- 但是我们也很容易发现这个精度的问题。我们假设一个简单的场景，如果我们只有几个神经元，y=0.5x + 0.6y + 0.7z + 0.8m + 0.9n，量化之后，这个方程变成什么了呢？y=1x + 1y + 1z + 1m + 1n。我们发现这个精度的表示太差了，对吧，所以我们需要有一些别的方式，把这个精度给补偿一下。我们发现乘以10之后，好像这个精度就能用整数表示了，对吧。y=1/10（5x + 6y + 7z + 8m + 9n），这样里面的这些参数，我们就可以用整数表示了，然后我们用一个fp32存一下外面的这个缩放因子就可以了。我们算一下这个状态下的压缩率啊。。。。这样呢，我们就在没有损失精度的前提下进行了压缩。
	-