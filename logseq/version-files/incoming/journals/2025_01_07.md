- {{embed ((677bc2b4-7870-4ea2-a6df-8e6752749c6a))}}
- # Lecture 6 Optimizing Optimizers
	- ![CUDAMODE_2024_Optimizing_optimizers.pptx.pdf](../assets/CUDAMODE_2024_Optimizing_optimizers.pptx_1736212536099_0.pdf)
	- Code reference:
		- https://github.com/NVIDIA/apex/blob/master/csrc/multi_tensor_apply.cuh
	- Kernel launch is expensive
	  collapsed:: true
		- ![image.png](../assets/image_1736215391358_0.png)
	- How to pass in a list of tensors addresses, the list is structure on CPU, and the list contains addresses on GPU
	  collapsed:: true
		- Pass float **, not work
			- CUDA 中尝试使用 C-style 数组（`float**`）时出现的问题：**非法内存访问 (Illegal Memory Access, IMA)**。
			- 因为传递过去的参数是CPU上的指针的list，GPU的kernel拿到这个list之后，去访问的地址是CPU的地址。如图所示，我们有多个list的GPU上的地址。然后我们把每个list的指针放到一个大的list里面。这个作为参数传递进GPU，但是这样GPU访问的还是CPU上的每个list的地址，会造成IMA。
				- ![image.png](../assets/image_1736222721832_0.png)
			- ![image.png](../assets/image_1736216373323_0.png)
		- CUDA的kernel function参数传入的时候，可以是一个struct
			- 为什么传递struct到kernel function是可以的？
				- 因为如下图所示，传递的是整个的Struct结构的数据。
				- ![image.png](../assets/image_1736222996164_0.png)
			- ![image.png](../assets/image_1736219772404_0.png)
		- 传递struct的问题：参数的大小有4k的限制
			- 是的，在 CUDA 编程中，**Kernel 参数空间**（Kernel Argument Space）确实有一个大小限制，通常最大为 **4KB**。这是设计上的限制，因为 CUDA 内核的参数是通过特定的 **快速内存区域**（寄存器或者常量内存）传递给内核的，而这个区域的大小是有限的。
			- ![image.png](../assets/image_1736219822902_0.png)
		- Solution to the 4K constraint:
			- Solution A:
				- launch multiple kernels where each kernel parameter is under constraint of 4K
				- Issue: kernel launch is expensive
				- ![image.png](../assets/image_1736223545492_0.png)
			- Solution B:
				- if the struct fits in 4K constraint, use struct
				- otherwise, take the memcpy hit, 把三个list of CUDA pointers 拷贝到GPU上，然后就可以构建一个GPU上的pointer of pointer。因为原本的List of [CPU pointers] 变成了List of [GPU pointers]。
				- ![image.png](../assets/image_1736220649762_0.png)
			- Cost comparison of the two solutions:
				- Memcpy cost vs Multiple kernel launch overhead
	- [[unified memory architecture]]
	- [[zero copy]]
	- [[torch.compile]]
- # Lecture 7 Advanced Quantization
	- ![Quantization Cuda vs Triton.pdf](../assets/Quantization_Cuda_vs_Triton_1736228513711_0.pdf)
	- ## Dynamic quantization
		- ![image.png](../assets/image_1736233939488_0.png)
	- ## Takeaways for dynamic quantization on GPU
		- Better per-token/per-channel quant on GPU since it took almost same effort as per-tensor quant
		- 6~7% perf improvement but ==Noticeable more== peak memory
			- reason: need int32 for intermediate accumulation instead of fp16
		- ![image.png](../assets/image_1736233994331_0.png)
		- Solution:
			- Add `Sw` (which is bf16) multiplication to the integer multiplication
			- ![image.png](../assets/image_1736234096404_0.png)
		- What happens when [[torch.compile]] a matrix multiplication?
			- https://github.com/pytorch/pytorch/blob/main/torch/_inductor/kernel/mm.py#L63C1-L63C13
			- Look for template and search for optimal configurations.
	- ## Int8 weight only quantization
		- ![image.png](../assets/image_1736236882340_0.png)
		- Naive idea: convert int8 to bf16 and reuse similar kernel as all quantization
			- slower than doing a non-quantized matmul
			- ![image.png](../assets/image_1736239451855_0.png)
		- Why slower?
			- 猜想：Batch size == 1, 导致tensorcore underutilized。
				- 在 **向量-矩阵乘法（batch size = 1）** 的场景中使用 Tensor Core 的问题主要包括：
					- **硬件利用率低**：向量的低维度无法充分激活 Tensor Core 的矩阵乘法单元。
					- **数据对齐与填充开销**：向量形状无法直接适配 Tensor Core 的瓦片对齐要求。
					- **访存效率低**：向量的访存模式无法充分利用片上缓存和内存带宽。
					- **适配成本高**：额外的填充和转换操作可能抵消 Tensor Core 的性能优势。
			- ![image.png](../assets/image_1736239527796_0.png)
		- How to solve this?
		  collapsed:: true
			- use the following to avoid running the code on tensor core
			- vector to matrix multiplication and use separate thread for each column
			- Faster kernel?
				- ![image.png](../assets/image_1736239709778_0.png)
				- Key notation:
					- The following code create 4096 threads, each column in the W dimension corresponds to one thread. XBLOCK always equal to 1
					- ```python
					  import triton
					  import triton.language as tl
					  
					  @triton.jit
					  def kernel(
					      in_ptr0, in_ptr1, in_ptr2, in_ptr3,  # 输入张量的指针
					      out_ptr1,                            # 输出张量的指针
					      xnumel, rnumel,                      # X 和 R 维度总元素数
					      XBLOCK: tl.constexpr,                # 每个线程块处理的 X 维度大小
					      RBLOCK: tl.constexpr                 # 每个线程块处理的 R 维度大小
					  ):
					    	
					      '''
					      XBLOCK always equal to 1,
					      也就是计算是类似于：
					      	1x4096 * 4096*4096 ==> 1x4096
					      每个线程计算的数值是1x1, 具体计算：
					      	1x4096 * 4096 * 1
					      '''
					      xnumel = 4096
					      rnumel = 4096
					      xoffset = tl.program_id(0) * XBLOCK
					      xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
					      xmask = xindex < xnumel
					      rbase = tl.arange(0, RBLOCK)[None, :]
					      x0 = xindex
					      _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
					      for roffset in range(0, rnumel, RBLOCK):
					          rindex = roffset + rbase
					          rmask = rindex < rnumel
					          r1 = rindex
					          tmp0 = tl.load(in_ptr0 + (r1), None, eviction_policy='evict_last').to(tl.float32)
					          tmp2 = tl.load(in_ptr1 + (r1 + (4096*x0)), xmask, eviction_policy='evict_first', other=0.0)
					          tmp2 = tmp0.to(tl.float32)
					          tmp3 = tmp2.to(tl.float32)
					          tmp4 = tmp1 * tmp3 # 求积
					          tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
					          _tmp6 = _tmp6 + tmp5
					      tmp6 = tl.sum(_tmp6, 1)[:, None] # 内积求和
					      tmp7 = tl.where(xmask, tmp7, _tmp6)
					      tmp9 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
					      tmp10 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last').to(tl.float32)
					      tmp11 = tmp8 * tmp9 # 可能是后面的scale
					      tmp12 = tmp11 + tmp10 # 可能是后面的bias
					      tl.store(out_ptr1 + (x0), tmp12, xmask)
					  ```
		- Int4 support
		  collapsed:: true
			- ![image.png](../assets/image_1736328893299_0.png)
		- Packing and unpacking of Int4
		  collapsed:: true
			- ![image.png](../assets/image_1736329934986_0.png)
			- 例子：
				- **Pack 过程**
					- 原始矩阵：
						- ```
						  [ A, B ]    ===>    [ 2, -3 ]
						  [ C, D ]            [ 5, -8 ]
						  [ E, F ]            [ 7,  0 ]
						  [ G, H ]            [-4,  1 ]
						  ```
					- 加偏移量：
						- ```
						  [ 10,  5 ]
						  [ 13,  0 ]
						  [ 15,  8 ]
						  [  4,  9 ]
						  ```
					- 打包成uint8：
						- ```
						  [ (5 << 4) | 10 = 0x52 ]
						  [ (0 << 4) | 13 = 0x0D ]
						  [ (8 << 4) | 15 = 0x8F ]
						  [ (9 << 4) |  4 = 0x94 ]
						  ```
				- **Unpack 过程**
					- 从 `uint8` 解包：
						- ```
						  [ 0x52, 0x0D, 0x8F, 0x94 ]
						  ```
					- 提取并减去偏移量 (-8)：
						- ```
						  [ 2, -3 ]
						  [ 5, -8 ]
						  [ 7,  0 ]
						  [-4,  1 ]
						  ```
	- ## [[Triton]] limitation
		- ![image.png](../assets/image_1736330390377_0.png)
	- ## Triton strengths
		- ![image.png](../assets/image_1736330435902_0.png)
- [[2025计划]]
- [[Triton]]