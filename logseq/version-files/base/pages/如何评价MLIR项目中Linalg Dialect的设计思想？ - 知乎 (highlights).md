title:: 如何评价MLIR项目中Linalg Dialect的设计思想？ - 知乎 (highlights)
author:: [[named ops]]
full-title:: "如何评价MLIR项目中Linalg Dialect的设计思想？ - 知乎"
category:: #articles
url:: https://www.zhihu.com/question/442964082/answer/1722372800
- #linalgdialect
- Highlights first synced by [[Readwise]] [[Sep 11th, 2023]]
	- 其实可以做一个类比。MLIR core 相当于编译器的 C++。那 MLIR 代码库里面的 dialect 就是 STL。能够提供一套高效的共用库是语言成功的重要因素。（从这个角度考虑，end-to-end 编译器相当于用 C++ 和 STL 写成的具体项目。）从这个角度来考虑，能在 MLIR core 里面存在的 dialect 也是要过一个很高的标准的。MLIR 的正确食用方式是用其提供的基础工具和共用模块，再加上自己的逻辑穿针引线，来实现一个 end-to-end 编译器。 ([View Highlight](https://instapaper.com/read/1632226639/23057303))
	- MLIR core 代码库里面其实没有必要存在任何的 dialect。MLIR core 对 out-of-tree dialect 的支持是其非常重要的设计根本，现在也比较完善。那 dialect 在MLIR core 里面存在的意义是什么呢？我个人认为有几点：1）提供复用的模块，2）让协作更容易，3）以及驱动 MLIR core 本身的演进。 ([View Highlight](https://instapaper.com/read/1632226639/23057308))
	- ML 编译器的任务是把开发者写的高层次的 model 转化成贴合底层硬件架构的指令。这里对成熟的 CPU/GPU 和正在飞速发展的 domain-specific acclerator 要分开来看。对 CPU/GPU，基本就是产生各种嵌套的 loop 和 tile，针对 CPU/GPU 的不同层次的 compute/memory hierarchy。如果我们把整个栈从上到下划分一下，大体上就是前端高层次算子，中层 loop，下层 control flow，vector 和 scalar。MLIR 的 progressive lowering 目的就是在各个层级创建合适的 abstraction 来解决现在很多 ML framework 里面一步到位的问题。 ([View Highlight](https://instapaper.com/read/1632226639/23057326))
	- Linalg dialect 基本就处于高于中层 loop 的位置。这里的 surface area 非常大，可以探讨的设计可能性非常多。传统的 loop 优化已经有很多的很好的既有经验了，大家可以参见像是《Optimizing Compilers for Modern Architectures》等书。 ([View Highlight](https://instapaper.com/read/1632226639/23057332))
	- Linalg 里面真正核心的 op 其实就是 linalg.generic 和 linalg.indexed_generic 两个。（后面统一称作 Linalg generic op。）这本身就让 transformation 非常容易写，因为基本只需要考虑这两个 op。Linalg generic op 本质是多层完美嵌套循环（perfect loop nest）的 op 化表示。Linalg generic op 里面用 indexing map 来隐性表示每层循环与输入输出的 access 关系，用附加的 region 表示针对这些输入输出进行的计算。这种设计从 IR 构建上就解决了很多传统 loop 优化的问题。因为根据定义，Linalg generic op 就是完美嵌套循环。针对 loop 做各种 transformation 的时候不可能存在非完美的情况，这样可以取消用来检测和维持 loop 完美性的逻辑。这可以算是 transformation 优先的一点展现吧。Transformation 会变得更易写、易读、易维护。使用 indexing map 来隐性表示每层循环与输入输出的关系也是让 transformation 优先的一点展现。举个例子，传统 loop 优化中如果要 fuse 两个 perfectly nested loop，需要分析每层 loop 的 induction variable 的 lower/upper bound 和 step，这些 induction variable 在 loop nest 里面 access 的 element，非常复杂。用 Linalg generic ops，只需要对 indexing map 进行 affine 操作：`inverse(producerIndexingMap).compose(consumerIndexingMap)`。（具体不再展开，想了解细节可以参见 Linalg Fushion 的代码。）其他的 transformation，像是 interchange、tiling、distribution 等等，也变得很简单，就不再一一举例了。 ([View Highlight](https://instapaper.com/read/1632226639/23057342))
	- Linalg dialect 中还有一个比较重要的概念，named ops。named ops 基本就是 generic ops 上面提供的 sugar：每个 named op 都有明确的隐性的 indexing map 和 compute region，它们定义了一个 named op。named ops 是可以和 generic op 相互转换的。（代码非常简单，不过现在只实现了 named ops 到 generic ops 的转换。）Named ops 存在的作用是和上层对接变得简单。算子层到 Linalg 层可以直接产生这些 named ops。但是在 Linalg 以及以下的层次上，transformation 主要操作的是 generic ops。确切地说是 generic ops 背后的 op interface。这样，我们可以狂加 named ops，但是编译器 transformation 确不需要修改，因为这些 named ops 都有同样的 op interface，既有 transformation 可以直接操作。这也是 transformation 优先的一点体现。（当然也不是说 transformation 完全不能操作 named ops。如果有针对某一 named op 的优化，也完全可以 match 那个 named op 然后写 transformation。这就是 Linalg generic op 到 named op 转换的初衷：我们可以随时在两种形态中变化。）除此之外，named ops 也是 CodeGen 中利用现有手写 kernel 库的途径。 ([View Highlight](https://instapaper.com/read/1632226639/23057353))
-