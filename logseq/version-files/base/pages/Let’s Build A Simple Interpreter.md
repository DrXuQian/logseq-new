---
title: Let’s Build A Simple Interpreter
---
- Commitment:
	 - I, **DrQianXu**, of being sound mind and body, do hereby pledge to commit to studying interpreters and compilers starting today and get to a point where I know 100% how they work!
	 - __Signature:__
		 - ![](../assets/MKlZ6WJJvf.png)
	 - __Date:__
		 - 2021/08/31
		 - ![](https://ruslanspivak.com/lsbasi-part1/lsbasi_part1_commitment_pledge.png)
- Notes:
	 - [Let’s Build A Simple Interpreter. Part 1.](https://ruslanspivak.com/lsbasi-part1/)
		 - **Interpreter and compiler**
			 - > At this point you may also wonder what the difference is between an interpreter and a compiler. For the purpose of this series, let’s agree that if a translator translates a source program into machine language, it is a **compiler**. If a translator processes and executes the source program without translating it into machine language first, it is an **interpreter**. Visually it looks something like this:
		 - Difference between a compiler and interpreter
			 - ![](https://ruslanspivak.com/lsbasi-part1/lsbasi_part1_compiler_interpreter.png)
		 - **Token**
			 - > When you enter an expression __3+5__ on the command line your interpreter gets a string __“3+5”__. In order for the interpreter to actually understand what to do with that string it first needs to break the input __“3+5”__ into components called **tokens**. A **token** is an object that has a type and a value. For example, for the string __“3”__ the type of the token will be INTEGER and the corresponding value will be integer __3__.
		 - **Lexer**
			 - > The process of breaking the input string into tokens is called **lexical analysis**. So, the first step your interpreter needs to do is read the input of characters and convert it into a stream of tokens. The part of the interpreter that does it is called a **lexical analyzer**, or **lexer** for short. You might also encounter other names for the same component, like **scanner** or **tokenizer**. They all mean the same: the part of your interpreter or compiler that turns the input of characters into a stream of tokens.
		 - **Check your understanding**
			 - What is an interpreter?
				 - interpreter can execute the language without translating into machine code.
			 - What is a compiler?
				 - compiler need to translate the language to machine code before execution.
			 - What’s the difference between an interpreter and a compiler?
				 - as the upper two shows.
			 - What is a token?
				 - token has type and value, i.e. INTERGER with value 3.
			 - What is the name of the process that breaks input apart into tokens?
				 - lexical analysis
			 - What is the part of the interpreter that does lexical analysis called?
				 - lexer
			 - What are the other common names for that part of an interpreter or a compiler?
				 - tokenizer
	 - [Let’s Build A Simple Interpreter. Part 2.](https://ruslanspivak.com/lsbasi-part2/)
		 - **Lexeme**
			 - >  A lexeme is a sequence of characters that form a token.
			 - ![](https://ruslanspivak.com/lsbasi-part2/lsbasi_part2_lexemes.png)
		 - **Parser**
			 - > The process of finding the structure in the stream of tokens, or put differently, the process of recognizing a phrase in the stream of tokens is called **parsing**. The part of an interpreter or compiler that performs that job is called a **parser**.
			 - **Check your understanding.**
				 - What is a lexeme?
					 - lexeme is a sequence of chars that form a token
				 - What is the name of the process that finds the structure in the stream of tokens, or put differently, what is the name of the process that recognizes a certain phrase in that stream of tokens?
					 - parsing
				 - What is the name of the part of the interpreter (compiler) that does parsing?
					 - expr
	 - [Let’s Build A Simple Interpreter. Part 3.](https://ruslanspivak.com/lsbasi-part3/)
		 - **Syntax diagram**
			 - ![](https://ruslanspivak.com/lsbasi-part3/lsbasi_part3_syntax_diagram.png)
			 - > What is a syntax diagram? A **syntax diagram** is a graphical representation of a programming language’s syntax rules. Basically, a syntax diagram visually shows you which statements are allowed in your programming language and which are not.
		 - **Check your understanding.**
			 - What is a syntax diagram?
				 - graphical representation of the syntax rule (much like ast)
			 - What is syntax analysis?
				 - Parsing
			 - What is a syntax analyzer?
				 - Parser
	 - [Let’s Build A Simple Interpreter. Part 4.](https://ruslanspivak.com/lsbasi-part4/)
		 - **quote**
			 - > 不闻不若闻之，闻之不若见之，见之不若知之，知之不若行之；学至于行之而止矣。
		 - **context-free grammars**
			 - another widely used notation for specifying the syntax of a programming language.
			 - example
				 - Here is a grammar that describes arithmetic expressions like “7 * 4 / 2 * 3” (it’s just one of the many expressions that can be generated by the grammar):
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_bnf1.png)
				 - A grammar consists of a sequence of __rules__, also known as __productions__. There are two rules in our grammar:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_bnf2.png)
				 - A rule consists of a __non-terminal__, called the **head** or **left-hand side** of the production, a colon, and a sequence of terminals and/or non-terminals, called the **body** or **right-hand side** of the production:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_bnf3.png)
				 - In the grammar I showed above, tokens like MUL, DIV, and INTEGER are called **terminals** and variables like __expr__ and __factor__ are called **non-terminals**. Non-terminals usually consist of a sequence of terminals and/or non-terminals:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_bnf4.png)
				 - The non-terminal symbol on the left side of the first rule is called the **start symbol**. In the case of our grammar, the start symbol is __expr__:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_bnf5.png)
				 - You can read the rule __expr__ as “An __expr__ can be a __factor__ optionally followed by a __multiplication__ or __division__ operator followed by another __factor__, which in turn is optionally followed by a __multiplication__ or __division__ operator followed by another __factor__ and so on and so forth.”
				 - What is a __factor__? For the purpose of this article a __factor__ is just an integer.
				 - Let’s quickly go over the symbols used in the grammar and their meaning.
					 - **|** - Alternatives. A bar means “or”. So (MUL | DIV) means either MUL or DIV.
					 - **( … )** - An open and closing parentheses mean grouping of terminals and/or non-terminals as in (MUL | DIV).
					 - **( … )*** - Match contents within the group zero or more times.
				 - A grammar defines a __language__ by explaining what sentences it can form. This is how you can __derive__ an arithmetic expression using the grammar: first you begin with the start symbol __expr__ and then repeatedly replace a non-terminal by the body of a rule for that non-terminal until you have generated a sentence consisting solely of terminals. Those sentences form a __language__ defined by the grammar.
				 - If the grammar cannot derive a certain arithmetic expression, then it doesn’t support that expression and the parser will generate a syntax error when it tries to recognize the expression.
				 - I think a couple of examples are in order. This is how the grammar derives the expression __3__:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_derive1.png)
				 - This is how the grammar derives the expression __3 * 7__:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_derive2.png)
				 - And this is how the grammar derives the expression __3 * 7 / 2__:
				 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_derive3.png)
		 - **Guideline for designing parser with grammars**
			 - Here are the guidelines that we will use to convert the grammar to source code. By following them, you can literally translate the grammar to a working parser:
				 - Each rule, **R**, defined in the grammar, becomes a method with the same name, and references to that rule become a method call: **R()**. The body of the method follows the flow of the body of the rule using the very same guidelines.
				 - Alternatives **(a1 | a2 | aN)** become an **if-elif-else** statement
				 - An optional grouping **(…)*** becomes a **while** statement that can loop over zero or more times
				 - Each token reference **T** becomes a call to the method **eat**: **eat(T)**. The way the __eat__ method works is that it consumes the token __T__ if it matches the current __lookahead__ token, then it gets a new token from the lexer and assigns that token to the __current_token__ internal variable.
			 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_rules.png)
		 - **Check your understanding.**
		 - Keeping in mind the grammar from today’s article, answer the following questions, referring to the picture below as needed:
		 - ![](https://ruslanspivak.com/lsbasi-part4/lsbasi_part4_bnf1.png)
			 - What is a context-free grammar (grammar)?
				 -
			 - How many rules / productions does the grammar have?
				 - 2
			 - What is a terminal? (Identify all terminals in the picture)
				 - MUL/DIV/INTERGER
			 - What is a non-terminal? (Identify all non-terminals in the picture)
				 - expr/factor
			 - What is a head of a rule? (Identify all heads / left-hand sides in the picture)
				 - expr/factor
			 - What is a body of the rule? (Identify all bodies / right-hand sides in the picture)
				 - left..
			 - What is the start symbol of a grammar?
				 - expr
	 - [Let’s Build A Simple Interpreter. Part 5.](https://ruslanspivak.com/lsbasi-part5/)
		 -