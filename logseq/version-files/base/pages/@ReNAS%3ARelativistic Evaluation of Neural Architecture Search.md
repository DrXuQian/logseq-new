tags:: [[Computer Science - Machine Learning]], [[Computer Science - Neural and Evolutionary Computing]]
date:: [[Sep 22nd, 2021]]
extra:: arXiv: 1910.01523
title:: @ReNAS:Relativistic Evaluation of Neural Architecture Search
item-type:: [[journalArticle]]
access-date:: 2022-03-07T03:25:19Z
original-title:: ReNAS:Relativistic Evaluation of Neural Architecture Search
url:: http://arxiv.org/abs/1910.01523
short-title:: ReNAS
publication-title:: arXiv:1910.01523 [cs]
authors:: [[Yixing Xu]], [[Yunhe Wang]], [[Kai Han]], [[Yehui Tang]], [[Shangling Jui]], [[Chunjing Xu]], [[Chang Xu]]
library-catalog:: arXiv.org
links:: [Local library](zotero://select/library/items/5KANB584), [Web library](https://www.zotero.org/users/9063164/items/5KANB584)
- [[Abstract]]
	- An effective and efficient architecture performance evaluation scheme is essential for the success of Neural Architecture Search (NAS). To save computational cost, most of existing NAS algorithms often train and evaluate intermediate neural architectures on a small proxy dataset with limited training epochs. But it is difficult to expect an accurate performance estimation of an architecture in such a coarse evaluation way. This paper advocates a new neural architecture evaluation scheme, which aims to determine which architecture would perform better instead of accurately predict the absolute architecture performance. Therefore, we propose a \textbf{relativistic} architecture performance predictor in NAS (ReNAS). We encode neural architectures into feature tensors, and further refining the representations with the predictor. The proposed relativistic performance predictor can be deployed in discrete searching methods to search for the desired architectures without additional evaluation. Experimental results on NAS-Bench-101 dataset suggests that, sampling 424 ($0.1\%$ of the entire search space) neural architectures and their corresponding validation performance is already enough for learning an accurate architecture performance predictor. The accuracies of our searched neural architectures on NAS-Bench-101 and NAS-Bench-201 datasets are higher than that of the state-of-the-art methods and show the priority of the proposed method.
- [[Notes]]
	- Comment: CVPR 2021, Oral
- tags:: [[Computer Science - Machine Learning]], [[Computer Science - Neural and Evolutionary Computing]]
  date:: [[Sep 22nd, 2021]]
  extra:: arXiv: 1910.01523
  title:: @ReNAS:Relativistic Evaluation of Neural Architecture Search
  item-type:: [[journalArticle]]
  access-date:: 2022-03-07T03:25:19Z
  original-title:: ReNAS:Relativistic Evaluation of Neural Architecture Search
  url:: http://arxiv.org/abs/1910.01523
  short-title:: ReNAS
  publication-title:: arXiv:1910.01523 [cs]
  authors:: [[Yixing Xu]], [[Yunhe Wang]], [[Kai Han]], [[Yehui Tang]], [[Shangling Jui]], [[Chunjing Xu]], [[Chang Xu]]
  library-catalog:: arXiv.org
  links:: [Local library](zotero://select/library/items/5KANB584), [Web library](https://www.zotero.org/users/9063164/items/5KANB584)
- [[Abstract]]
	- An effective and efficient architecture performance evaluation scheme is essential for the success of Neural Architecture Search (NAS). To save computational cost, most of existing NAS algorithms often train and evaluate intermediate neural architectures on a small proxy dataset with limited training epochs. But it is difficult to expect an accurate performance estimation of an architecture in such a coarse evaluation way. This paper advocates a new neural architecture evaluation scheme, which aims to determine which architecture would perform better instead of accurately predict the absolute architecture performance. Therefore, we propose a \textbf{relativistic} architecture performance predictor in NAS (ReNAS). We encode neural architectures into feature tensors, and further refining the representations with the predictor. The proposed relativistic performance predictor can be deployed in discrete searching methods to search for the desired architectures without additional evaluation. Experimental results on NAS-Bench-101 dataset suggests that, sampling 424 ($0.1\%$ of the entire search space) neural architectures and their corresponding validation performance is already enough for learning an accurate architecture performance predictor. The accuracies of our searched neural architectures on NAS-Bench-101 and NAS-Bench-201 datasets are higher than that of the state-of-the-art methods and show the priority of the proposed method.
- [[Notes]]
	- Comment: CVPR 2021, Oral