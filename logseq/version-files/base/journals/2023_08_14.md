- Script for dynamic shape support in Nbperf:
	- Slide 8:
		- In the backend, we can support dynamic shape by tiling on the scalable dimension.
		- One thing to note is that there is no tiling on the scalable dimension if we have reduce operation on that dims. For example, if the scalable dimension is the input channel of a Matmul or the normalization dim of Softmax, we can't tile that dimension because there are reduction on these dims.
		- For example, for the following matrix multiply,  [1,seq, hidden] * [1, hidden, hidden] --> [1,seq, hidden]. The sequence length is the dynamic dimension. After tiling, we tile the sequence length to 32, which is a hyper parameter we set. And we would have sequence length // 32 number of  tiled matmuls. And for each matmul, the shape is [1, 32, hidden] * [1, hidden, hidden] --> [1, 32, hidden]
	- Slide 9:
		- And for the example in the previous slide, suppose the sequence length is 128 and we tile 128 to 4 matmul, each with 32 as sequence length. Then we have the following graph. And for each matmul, we have an output offset in memory space with respect to the output for the whole sequence. For the first tiled op, the offset is off by 0, And for the second, the offset is off by 32 in the scalable dimension. For the third, off by 64 and for the fourth, off by 96.
		- And suppose we have 64 as sequence length when we are doing the actual inference. We would skip the latter two matmuls since these matmul offset are over the limit.
	- Slide 10:
		- Similarly, we can extend the example to support dynamic shape for transformer structure. We can also tile the sequence length dimension, which is the scalable dimension for transformers.
		- As you can see in the bottom right figure, the transformer structure consists of the multi-head attention and the feed forward layer and the layer norms in between these components. For multi-head attention, we first calculate the linear embedding for Q, K, V. For these matrix multiply, We can also tile on the sequence length dimension just as we showed in previous slides and then we get Q, K and V for 32 tokens at a time.
		- For scaled dot-product attention, it consists of a matmul followed by a softmax followed by another matmul. For this structure, we can still partition Q in the sequence length dimension but we need the K and V for whole sequence, this is because when calculating the attention score, we are calculating against all tokens. Because of this, we need to collect all K and V before calculating the attention score using the first matmul and softmax and the attention embedding using the second matmul.
		- For the feed forward layer, we can tile on the sequence length dimension without any issue since these are just calculation of the embedding for different tokens. Same goes for the normalization layer since the normalization didn't touch the sequence length dimension.
	- Slide 11:
		- In this slide, we translate the transformer tiling to the following pseudo code. For a transformer, we first split on the sequence length dimension and calculate the corresponding Q, K and V for the tiled tokens.
		- After that we do dot product attention, first we collect all K and V, and then for the previous generated Qs, we calculate the corresponding attention score and attention embedding with the whole K and V. After that, the attention embedding go through the feed forward layer and after that is anther block and we repeat the process.
		- As you can see, the S is the outer loop and we can skip dynamically during runtime.
	- Slide 12:
		- The following graph shows a structure of a transformer after applying the method we discussed.
		- As you can see, we partitioned the sequence length dimension to 32 tokens for each computation. We first calculate Q, K, V for the 32 tokens. And then, we collect K, V for whole sequence. Then we calculate the attention embedding for the 32 tokens' query using the dot-product attention. And then followed by the feedforward layer, And then we calculate the Q, K and V for the 32 tokens and repeat the process.
		- As you see, for the dot-product attention and the feed forward layer and the Q K V for the next block. We can tile the sequence length to 32 and vertically fuse these components. And we can skip the unused compute block during runtime since the S is the outer loop. One thing to note is that we need to collect K, V for all tokens in between these compute blocks.
-