- [[2025计划]]
- [[PMPP]]
- [[float4 in CUDA]]
- [[kernel: Relu and Relu vec4]]
- [[CUDA Mode]]
	- # Lecture 1 How to profile CUDA kernels in PyTorch
	  collapsed:: true
		- [[torch profiler]]
		  collapsed:: true
			- ![image.png](../assets/image_1736133699885_0.png){:height 102, :width 530}
			- port to a trace
			  collapsed:: true
				- ![image.png](../assets/image_1736133717917_0.png)
		- Custom cpp extensions in pytorch
		  collapsed:: true
			- load_inline
				- 自动生成cpp文件和makefile
			- ![image.png](../assets/image_1736133832173_0.png){:height 361, :width 469}
			-
		- [[Triton]]
			- takeaways:
				- 1. DSL in python
				- 2. Will generate PTX kernel instead of a CUDA kernel
				- 3. Triton have a very nice debugger that enables line by line debugging
					- ![image.png](../assets/image_1736134379367_0.png)
		- [[torch.compile]]
			- ![image.png](../assets/image_1736134614067_0.png)
		-
	- # Lecture 5: Going Further with CUDA for Python Programmers
	  id:: 677bc2b4-7870-4ea2-a6df-8e6752749c6a
		- How to create [[dynamic shared memory]] in CUDA kernels? #card
		  collapsed:: true
			- `extern __shared__` 关键字，大小可以动态配置。
			- 静态的shared memory 必须在编译时指定大小。
			- ![image.png](../assets/image_1736211013366_0.png){:height 388, :width 623}
			- ```c++
			  __global__ void matmul_k(float *m, float *n, float *out, int h, int w, int k, int tw) {
			      int tc=threadIdx.x, tr=threadIdx.y;
			      int r=blockIdx.y*blockDim.y+tr, c=blockIdx.x*blockDim.x+tc;
			  
			      extern __shared__ float ms[];
			      float *ns = &ms[tw*tw];
			  
			      float p = 0.0f;
			      for (int ph = 0; ph < cdiv(k,tw); ++ph) {
			          int idx = ph*tw;
			          ms[tr*tw + tc] = r<h && idx+tc<k ? m[ tc+idx + r*k ] : 0.0f;
			          ns[tr*tw + tc] = c<w && idx+tr<k ? n[(tr+idx)*w + c] : 0.0f;
			          __syncthreads();
			          for (int i=0; i<tw; ++i) p += ms[tr*tw + i] * ns[tw*i + tc];
			          __syncthreads();
			      }
			      if (r<h && c<w) out[r*w + c] = p;
			  }
			  ```
			-
		- 输出写出的时候是否有必要也创建shared memory，然后统一写出？#card
		  collapsed:: true
			- 将输出结果暂存到 **Shared Memory** 然后统一写入全局内存是一种有效的优化策略，特别是在以下场景下：
				- 需要对输出结果进行合并、归约或重新排列以优化全局内存访问模式。
				- 输出结果的写入模式不连续（non-coalesced）。
				- 输出数据较小，可以完全存储在 Shared Memory 中。
			- 如果输出结果本身是连续的，或全局内存写操作已经很高效，则可以直接写入全局内存，避免额外的同步开销。具体优化效果需要根据实际场景和硬件架构进行测试和分析！
	- [[numba]] write cuda code in python
	-