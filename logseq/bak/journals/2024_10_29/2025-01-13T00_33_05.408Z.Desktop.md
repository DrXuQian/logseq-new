- [[LLM related interview]]
	- RMS norm
	- Rope
	- positional embedding
- [[Refresh memory for architect interview]]
	- tops
		- MTL
			- 11Tops
		- LNL
			- 48Tops
	- DMA bw
		- MTL 64--> LNL 128
	- fps of some representative networks
		- stable diffusion
			- negative prompt and positive prompt run separately on NPU and CPU
			- text encoder and VAE also run on CPU
			- ![Stable Diffusion Inference Process](https://miro.medium.com/v2/resize:fit:1156/1*ka4ci_UymoxuH4LAjiA6iw.png)
	- CIM
		- NOW missing insights on the difference bw SCL and CIM
		  :LOGBOOK:
		  CLOCK: [2024-10-29 Tue 10:30:57]
		  :END:
	- memory side cache
		- share the same cache with L3 cache of CPU, mainly for the reduction of POWER because no need to go to DDR any more. Still use DMA to carry the data to cache.
	- iti connection
		- use inter tile connection to carry halo region data to other tiles
		- thinking about improving this by adding another level of small cache shared by all tiles
		- MTL directly read from other tiles, LNL directly write to other tiles. Reason, reduce the memory  read from other tiles. halo region might be read for more than once.
	- MAC array
		- IDU-->SCL-->PPE-->ODU
		- adder bit width
		- 4x4x16 x 16x16 ==> 4x4x16
	- MMU and TLB
		- MMU for address translation, TLB for address translation cache
	- fp32/fp16/fp8
		- [[floating point]]
		- ![image.png](../assets/image_1730172676561_0.png)
		- exp 越大，范围越大
		- exp 越小，范围越小
		- ![image.png](../assets/image_1730172528596_0.png)
		- ![0f1b050d47544d4fa2b921fcf8b2a490](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/_images/fp8_formats.png)
	- software stack
	- DSP instruction width
		- MTL 128 ==> LNL 512
	- ULP constraint
		- **最后一位上的单位值**或称**最小精度单位**，缩写为**ULP**，是毗邻的浮点数值之间的距离，也即浮点数在保持指数部分的时候最低有效数字为1所对应的值。
	- accumulation context
	- swizzling
		- 1.5M ==> 32 bank, 48KB each bank
		- 2M ==> 32 bank, 64KB each bank
		- CMX read: 256B/Cycle
	- GPTQ
		- what is the group in GPTQ?
			- in GPTQ, a group of channels sharing the same quantization scale means that within each group, the same scale and zero-point are applied. This helps maintain consistency and reduces the complexity of computations. By grouping channels, you can optimize the balance between model size, speed, and accuracy.
	- TOPS/W
	- DMA bandwidth and DDR bandwidth
		- 32x2 ==> 64x2
	- What can be improved?
		- memory hierarchy
		- programming model
			- no direct control from the DSP to the MAC array
		- slim workload descriptor
		- workload management
		- more powerful shave/DSP
- [[What can be learned in HW?]]
  id:: 6729bffa-e289-4e7d-9b0b-3b4f6339bcdd
	- How huawei support dynamic inference graph?
	  collapsed:: true
		- [[动态图和静态图]]
		  collapsed:: true
			- Mindspore中有什么：
			  collapsed:: true
				- Mindspore有几种模式，动态图和静态图模式，这个是相对应于pytorch的静态图模式，以及tensorflow的动态图模式来说的。
			- ### 1.概论
			  collapsed:: true
				- **动态图：**比较方便debug，非常容易上手，但是优化部署较为不易。pytorch可以用python的语句进行编程，if else, while等等。
				- **静态图：**通过先定义后运行的方式，之后再次运行的时候就不再需要重新构建计算图，所以速度更快。同时，由于全局的计算图结构在开始时候就已经定义好，对于模型优化和部署更加友好，但对初学者而言不太友好。
				  collapsed:: true
					- ![image.png](../assets/image_1733386806637_0.png){:height 507, :width 559}
					- 静态图的优化：
					  collapsed:: true
						- ![image.png](../assets/image_1733387018699_0.png)
			- ### 2.代码层面的差异举例
			  collapsed:: true
				- Tensorflow循环：
				  collapsed:: true
					- ```python
					  # tensorflow
					  import tensorflow as tf
					  #1.定义变量
					  first_counter = tf.constant(0)
					  second_counter = tf.constant(10)
					  #2.定义条件函数
					  def cond(first_counter, second_counter, *args):
					      return first_counter < second_counter
					  #3.定义循环体
					  def body(first_counter, second_counter):
					      first_counter = tf.add(first_counter, 2)
					      second_counter = tf.add(second_counter, 1)
					      return first_counter, second_counter
					  #4.使用tf API完成while算子的生成
					  c1, c2 = tf.while_loop(cond, body, [first_counter, second_counter])
					  
					  with tf.Session() as sess:
					      counter_1_res, counter_2_res = sess.run([c1, c2])
					  print(counter_1_res)
					  print(counter_2_res)
					  
					  ```
					- 可以看到 TensorFlow 需要将整个图构建成静态的，换句话说，每次运行的时候图都是一样的，是不能够改变的，所以不能直接使用 Python 的 while 循环语句，需要使用辅助函数 `tf.while_loop` 写成 TensorFlow 内部的形式,这是非常反直觉的，学习成本也是比较高的.
				- Pytorch循环：
				  collapsed:: true
					- ```python
					  # pytorch
					  import torch
					  first_counter = torch.Tensor([0])
					  second_counter = torch.Tensor([10])
					  while (first_counter < second_counter)[0]:
					      first_counter += 2
					      second_counter += 1
					  print(first_counter)
					  print(second_counter)
					  ```
			- ### 3. 主流框架的支持情况
			  collapsed:: true
				- ![](https://pic4.zhimg.com/v2-632f98fbed423368e51ea4d3304267c9_1440w.jpg)
	- KV cache compression
		- [[H2O]]
		  collapsed:: true
			- [Heavy Hitter Oracle (H2O)](https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf)
			- Overall:
			  collapsed:: true
				- 该论文提出了一种名为Heavy Hitter Oracle (H2O)的新方法,旨在实现大型语言模型(LLM)中关键值(KV)高速缓存,从而显著降低在线推理过程中的内存占用。
				- 论文提出了两个关于LLM的关键观察结果:
				  collapsed:: true
					- 预训练的LLM在推理时注意力矩阵非常稀疏(稀疏度超过95%),这表明KV高速缓存的大小可以大幅减小。
					- 一小部分词元(称为"重要词元")对注意力分数的贡献占主导地位,对生成质量至关重要。
			- 背景介绍：
			  collapsed:: true
				- ![image.png](../assets/image_1733388803958_0.png)
			- 稀疏的来源：
			  collapsed:: true
				- 原始的attention score是没有稀疏度的，如果设置一个阈值进行筛选，这个会如何影响精度？
				  collapsed:: true
					- 应该是跟sparse transformer进行的比较：
					  collapsed:: true
						- Generating Long Sequences with Sparse Transformers
						  collapsed:: true
							- ![image.png](../assets/image_1733390873766_0.png)
					- 如图2a所示
					  collapsed:: true
						- ![image.png](../assets/image_1733388389855_0.png)
			- 重要词源的来源：
			  collapsed:: true
				- 不同的词源对于其他的词源的重要性是不一样的，如何进行筛选?
				  collapsed:: true
					- 贪心的方法，按列求和取topK。
				- ![](https://pic1.zhimg.com/v2-d5f479f692ba724dc850bba6330ab954_r.jpg){:height 537, :width 461}
			- 问题：
			  collapsed:: true
				- 贪心法肯定是有不足的。后来的文章也有批判这一点，如下图给了一个反例。贪心法只能得到先前的token关注哪些信息，有可能前面token不关心的信息，是后续的token关心的信息，而贪心法已经drop掉了，就会影响模型性能。
				- ![](https://pic1.zhimg.com/v2-28ef41ce1a17fd60895c65e4a3ba3002_r.jpg){:height 433, :width 471}
		- [[snap KV]]
		- power infer
		- pyramid infer/pyramid kv
		- razor attention
	- Must learn
		- [[PD 分离]]
		  collapsed:: true
			- MoonCake
		- [[Continuous Batching]]
		- [[speculative decoding]]
		- FasterTransformer
		- [[StreamingLLM]]
		- VLLM
			- FlashInfer/[[Flash attention]]
			- paged attention
			- memory sharing
		- SGlang
		-
- [[Prepare interview for Nvidia and AMD]]
	- GPU structure
	- Coding
	- C++
	- CUDA
	- TensorRT