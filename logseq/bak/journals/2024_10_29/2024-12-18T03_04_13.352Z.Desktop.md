- [[LLM related interview]]
	- RMS norm
	- Rope
	- positional embedding
- [[Refresh memory for architect interview]]
  collapsed:: true
	- tops
		- MTL
			- 11Tops
		- LNL
			- 48Tops
	- DMA bw
		- MTL 64--> LNL 128
	- fps of some representative networks
	  collapsed:: true
		- stable diffusion
			- negative prompt and positive prompt run separately on NPU and CPU
			- text encoder and VAE also run on CPU
			- ![Stable Diffusion Inference Process](https://miro.medium.com/v2/resize:fit:1156/1*ka4ci_UymoxuH4LAjiA6iw.png)
	- CIM
	  collapsed:: true
		- NOW missing insights on the difference bw SCL and CIM
		  :LOGBOOK:
		  CLOCK: [2024-10-29 Tue 10:30:57]
		  :END:
	- memory side cache
	  collapsed:: true
		- share the same cache with L3 cache of CPU, mainly for the reduction of POWER because no need to go to DDR any more. Still use DMA to carry the data to cache.
	- iti connection
	  collapsed:: true
		- use inter tile connection to carry halo region data to other tiles
		- thinking about improving this by adding another level of small cache shared by all tiles
		- MTL directly read from other tiles, LNL directly write to other tiles. Reason, reduce the memory  read from other tiles. halo region might be read for more than once.
	- MAC array
	  collapsed:: true
		- IDU-->SCL-->PPE-->ODU
		- adder bit width
		- 4x4x16 x 16x16 ==> 4x4x16
	- MMU and TLB
	  collapsed:: true
		- MMU for address translation, TLB for address translation cache
	- fp32/fp16/fp8
	  collapsed:: true
		- ![image.png](../assets/image_1730172676561_0.png)
		- exp 越大，范围越大
		- exp 越小，范围越小
		- ![image.png](../assets/image_1730172528596_0.png)
		- ![0f1b050d47544d4fa2b921fcf8b2a490](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/_images/fp8_formats.png)
	- software stack
	- DSP instruction width
		- MTL 128 ==> LNL 512
	- ULP constraint
		- **最后一位上的单位值**或称**最小精度单位**，缩写为**ULP**，是毗邻的浮点数值之间的距离，也即浮点数在保持指数部分的时候最低有效数字为1所对应的值。
	- accumulation context
	- swizzling
		- 1.5M ==> 32 bank, 48KB each bank
		- 2M ==> 32 bank, 64KB each bank
		- CMX read: 256B/Cycle
	- GPTQ
		- what is the group in GPTQ?
			- in GPTQ, a group of channels sharing the same quantization scale means that within each group, the same scale and zero-point are applied. This helps maintain consistency and reduces the complexity of computations. By grouping channels, you can optimize the balance between model size, speed, and accuracy.
	- TOPS/W
	- DMA bandwidth and DDR bandwidth
		- 32x2 ==> 64x2
	- What can be improved?
		- memory hierarchy
		- programming model
		- slim workload descriptor
		- workload management
		- more powerful shave/DSP
- [[What can be learned in HW?]]
	- How huawei support dynamic inference graph?
	- KV cache compression
		- H2O
		- snapkv
		- power infer
		- pyramid infer/pyramid kv
		- razor attention
	- Must learn
		- PD分离
			- MoonCake
		- continuous batching
		- speculative decoding
		- FasterTransformer
		- StreamingLLM
		- VLLM
			- FlashInfer/[[Flash attention]]
			- paged attention
			- memory sharing
		- SGlang
		-
- [[Prepare interview for Nvidia and AMD]]
	- GPU structure
	- Coding
	- C++
	- CUDA
	- TensorRT