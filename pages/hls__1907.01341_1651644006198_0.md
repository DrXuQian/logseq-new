file-path:: ../assets/1907.01341_1651644006198_0.pdf
file:: [1907.01341_1651644006198_0.pdf](../assets/1907.01341_1651644006198_0.pdf)
title:: hls__1907.01341_1651644006198_0
- Training on a single dataset leadsto good performance on the corresponding test split of the samedataset (same camera parameters, depth annotation, environment),but  may  have  limited  generalization  capabilities  to  unseen  datawith  different  characteristics.  Instead,  we  propose  to  train  on  acollection  of  datasets,  and  demonstrate  that  this  approach  leadsto strongly enhanced generalization by testing on diverse datasetsthat  were  not  seen  during  training.  We  list  our  training  and  testdatasets, together with their individual characteristics, in Table 1.
  ls-type:: annotation
  hl-page:: 3
  id:: 627520e5-5b36-49ef-adef-62c9f93ca4b9
- zero-shot cross-dataset transfer.
  ls-type:: annotation
  hl-page:: 3
  id:: 62752133-b3f1-4462-a465-39ade71b2d3e
- Mixing Datasets forZero-shot Cross-dataset Transfer
  ls-type:: annotation
  hl-page:: 1
  id:: 62752208-e780-4005-a93a-9ece899d8ace
- We  start  from  the  experimental  setup  of  Xianet al.[32]  anduse  their  ResNet-based  [56]  multi-scale  architecture  for  single-image depth prediction. We initialize the encoder with pretrainedImageNet  [57]  weights  and  initialize  other  layers  randomly.  Weuse  Adam  [58]  with  a  learning  rate  of10−4for  randomly
  ls-type:: annotation
  hl-page:: 6
  id:: 6275b331-42dc-4b67-8aec-d054ab4f5879
- initialized  layers  and10−5for  pretrained  layers,  and  set  theexponential decay rate toβ1= 0.9andβ2= 0.999. Images areflipped  horizontally  with  a  50%  chance,  and  randomly  croppedand resized to384×384to augment the data and maintain theaspect ratio across different input images. No other augmentationsare used.
  ls-type:: annotation
  hl-page:: 7
  id:: 6275b33d-210d-4823-b312-f1be9bed2ac6
- We use ResNeXt-101-WSLfor all subsequent experiments.
  ls-type:: annotation
  hl-page:: 8
  id:: 6275b3e1-288c-4953-85d9-8b997f5c2037
- For  ablation  studies  of  loss  andencoders, we use our held-out validation sets of RW (360 images),MD  (2,963  images  –  official  validation  set),  and  MV  (3,058images – see Table 2). For all training dataset mixing experimentsand  comparisons  to  the  state  of  the  art,  we  test  on  a  collectionof  datasets  that  were  never  seen  during  training:  DIW,  ETH3D,Sintel,  KITTI,  NYU,  and  TUM.  For  DIW  [34]  we  created  avalidation  set  of  10,000  images  from  the  DIW  training  set  forour ablation studies and used the official test set of 74,441 imageswhen  comparing  to  the  state  of  the  art.  For  NYU  we  used  theofficial test split (654 images). For KITTI we used the intersectionof the official validation set for depth estimation (with improvedground-truth depth [59]) and the Eigen test split [60] (161 images).For ETH3D and Sintel we used the whole dataset for which groundtruth  is  available  (454  and  1,064  images,  respectively).  For  theTUM dataset, we use thedynamicsubset that features humans inindoor environments [38] (1,815 images).
  ls-type:: annotation
  hl-page:: 7
  id:: 6279b2da-f73e-4efb-af96-c220ec6c5d4f
- we use theexperimental protocol ofzero-shot cross-dataset transfer. That is,we train a model on certain datasets and then test its performanceon other datasets that were never seen during training. The intu-ition is that zero-shot cross-dataset performance is a more faithfulproxy  of  “real  world”  performance  than  training  and  testing  onsubsets  of  a  single  data  collection  that  largely  exhibit  the  samebiases 
  ls-type:: annotation
  hl-page:: 1
  id:: 627ca3a4-4262-432d-b57f-810d3ca1265e
- Datasets  differ  in  captured  environments  andobjects  (indoor/outdoor  scenes,  dynamic  objects),  type  of  depthannotation (sparse/dense, absolute/relative depth), accuracy (laser,time-of-flight,  SfM,  stereo,  human  annotation,  synthetic  data),image quality and camera settings, as well as dataset size.
  ls-type:: annotation
  hl-page:: 3
  id:: 627ca737-fda3-4428-9b1f-9976f7c1d954
- We  identify  three  major  challenges.
  ls-type:: annotation
  hl-page:: 5
  id:: 627ca895-0180-4919-af8d-04b5fdd9737c
- Scale- and shift-invariant losses.
  ls-type:: annotation
  hl-page:: 5
  id:: 627ca8bf-910a-4f81-8b31-1d73154d17b6