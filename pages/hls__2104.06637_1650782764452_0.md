file-path:: ../assets/2104.06637_1650782764452_0.pdf
file:: [2104.06637_1650782764452_0.pdf](../assets/2104.06637_1650782764452_0.pdf)
title:: hls__2104.06637_1650782764452_0
- Our  proposed  DSTT  disentangles  thetask of learning spatial-temporal attention into 2 sub-tasks:one is for attending temporal object movements on differ-ent frames at same spatial locations, which is achieved bytemporally-decoupled Transformer block, and the other isfor  attending  similar  background  textures  on  same  frameof  all  spatial  positions,  which  is  achieved  by  spatially-decoupled  Transformer  block.
  ls-type:: annotation
  hl-page:: 1
  id:: 6264fa8e-af91-430b-93f1-2f2691650cab
- The overall framework of our proposed method is illus-trated  in  Fig.  3.   Given  a  corrupted  video  sequence  withits  corresponding  mask  sequence,  individual  frames  arefirst input into a frame-based hierarchical encoderGHEfordownsampling and embedding into spatial-temporal tokenvectors.  
  ls-type:: annotation
  hl-page:: 3
  id:: 62654dbf-36e1-4d79-9f94-c69b147cf7b8
- Althought= 5is chosen during train-ing,  using a largertduring inference will produce videoswith much better visual quality and temporal consistency.So  the  inference  speed  is  boosted  to  a  great  extent.   Fol-lowing STTN [36], we set the number of all stacked Trans-former blocks to8.   As for the stacking strategy,  we em-pirically make a temporally-decoupled block followed by aspatially-decoupled block and repeat this for4times.
  ls-type:: annotation
  hl-page:: 5
  id:: 6265e866-d2b0-4b2f-b913-29791126d358